{\rtf1\ansi\ansicpg1252\cocoartf1265\cocoasubrtf210
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww18460\viewh16420\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\f0\fs36 \cf0 \

\fs48 Tentative schedule by week
\fs36 \
\
Overall objective: demonstrate performance improvement over state-of-the-art implementations of classical and Strassen-like matrix multiplication algorithms on sequential and shared-memory (and/or distributed-memory) parallel machines with code that efficiently implements any fast matrix multiplication algorithm\
\

\b\fs28 specific objectives:
\b0 \
	- compare 3x3x3 rank 23 algorithms with Strassen-like algorithms\
	- examine performance improvement (and numerical degradation) of approximate algorithms\
	- show performance improvement of 3x3x6 rank 40 algorithm for square matrices\
	- show performance improvement for rectangular matrix multiplication
\fs24 \
\

\b June 	23	\
		
\b0 * Get up and running / orientation and training
\b \
		
\b0 *
\b  
\b0 Set up benchmarking harness for shared-memory code, get initial results on workstation
\b \
	30\
		
\b0 * Tune shared-memory code\
		* Determine scenarios for performance improvement (compared to classical, Strassen)\
		* Get up and running on supercomputer node (maybe Edison) for more reliable benchmarking\

\b July	7\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0 \cf0 		* Look for other fast matmul shared-memory codes to compare against\
		* Set outline, target venue for performance paper\
		* Implement common subexpression elimination on UVW matrices\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b \cf0 	14	
\b0 \
		* Finish sequential implementations with 3 variants of addition code (write-once, streaming, pairwise), each with/without CSE optimization\

\b 		
\b0 -- (Grey on travel most of week)
\b \
	21	
\b0 \
		* Finish shared-memory parallel code: 2 variants of multiplies (DFS, hybrid BFS/DFS) and relevant variants of additions\

\b 		
\b0 -- (Grey on vacation)
\b \
	28	
\b0 \
		* Start general distributed-memory implementation\
		* Benchmark CAPS on supercomputer (maybe Edison)\

\b 		
\b0 -- (Grey on vacation)\
		-- (Student Intern Symposium on Thursday)
\b \
Aug	4\
		
\b0 * Tune distributed-memory code\

\b 	11\
		
\b0 * Start writing meat of performance paper\

\b 	18\
		
\b0 * Start to benchmark distributed-memory code\

\b 	25\
		
\b0 * Start putting together talk\
		* Continue writing/editing performance paper\

\b Sept	1\
		
\b0 * Give practice talk\
		* Complete full draft of performance paper for Sandia tech reports\

\b 	8\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0 \cf0 		* Give talk to Sandians\
		* Wrap up, organize materials for later papers\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b \cf0 Secondary Objectives:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural

\b0 \cf0  - pursue nullstellensatz certificates to prove lower bounds on rank of matrix multiplication tensors\
 - implement and prove effectiveness of diagonal scaling and permutations to improve numerical stability of fast matrix multiplication algorithms}